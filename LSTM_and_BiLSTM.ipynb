{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM and BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPm8sKtPrUhbGY4bmWOfvNt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinram472/projects/blob/master/LSTM_and_BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2hu7kAlfUPb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0353bcd5-2760-42a4-a683-061b890315ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lcIF9KafWty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp gdrive/My\\ Drive/AI-In-Peer-Assessment/problems_expertiza_randomized1.csv ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o9sKPMkhH2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# To store data\n",
        "import pandas as pd\n",
        "\n",
        "# To use regular expressions\n",
        "import re\n",
        "\n",
        "#To load and save data\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXdrqHxQhKIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = pd.read_csv(\"problems_expertiza_randomized1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvAy-t0yhLkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = reviews.filter([\"REVIEW\",\"TAG\"])\n",
        "data = data.loc[data.REVIEW.apply(lambda x: not isinstance(x, (float, int)))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRPY7ICfhass",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e1b90877-4500-4f2f-9e41-f0318a133be4"
      },
      "source": [
        "data['TAG'].value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    5585\n",
              "0    5585\n",
              "Name: TAG, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYeL1wQKhiUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set, test_set = train_test_split(data, test_size=0.05, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mah52AEVhloL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_reviews = list(train_set[\"REVIEW\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELjaTwq0hoVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = list(train_set[\"TAG\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odHx7M8bhp0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(train_reviews)):\n",
        "  train_reviews[i] = re.sub('\\d',' ',train_reviews[i]) # Replacing digits by space\n",
        "  train_reviews[i] = re.sub(r'\\s+[a-z][\\s$]', ' ',train_reviews[i]) # Removing single characters and spaces alongside\n",
        "  train_reviews[i] = re.sub(r'\\s+', ' ',train_reviews[i]) # Replacing more than one space with a single space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0jn2A_5huDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8fbfad47-5e86-4657-b336-eabd164387ab"
      },
      "source": [
        "for i in range(len(train_reviews)):\n",
        "    if 'www.' in train_reviews[i] or 'http:' in train_reviews[i] or 'https:' in train_reviews[i] or '.com' in train_reviews[i]:\n",
        "        train_reviews[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_reviews[i])\n",
        "        \n",
        "        \n",
        "train_reviews[1:5]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The deployed link is not accessible. The readme file describes the process for just admin login but not for the consequent steps.',\n",
              " 'Yes, it is reasonable as I dont��see any obvious issues with it. But the testing could be done better rather than checking the previoustest��cases that was already present.',\n",
              " 'Basic functional test cases are written for four controllers.',\n",
              " 'The team has only made one significant change to the following file: app/controllers/assignments_controller.rb . It is an if statement which is difficult to follow since the team has not written any comments for the same.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3SQNJdrhzLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "b29fe935-32ec-4dbd-d103-abae94d805e5"
      },
      "source": [
        "# Use Tokenizer to remove punctuations and non-word characters and tokenize the text\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='tensorflow' # Or TenserFlow\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOVPYMh1h7px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 80\n",
        "MAX_NB_WORDS = 80 # This specifies how many top tokens in each review to be stored. Wrongly interpreted as total number of words(token) together in whole dataset\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O4D6w1qh_pV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d512a7c-6d34-41d9-989c-09de8db2efb3"
      },
      "source": [
        "# Takes 5 minutes to run on entire training dataset\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(train_reviews)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Number of Unique Tokens',len(word_index)) # Total 996497 unique words"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unique Tokens 8580\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeGAw9AUiCF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RApBkixlkLOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp gdrive/My\\ Drive/Glove/glove100d.txt ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pii11AwUiGc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('glove100d.txt')):\n",
        "  values = line.split() # 0 th index will be the word and rest will the embedding vector (size 100 as we have used Glove.6B.100D embedding file) \n",
        "  embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdGk43Ruim4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create token(words in word index)-embedding mapping\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100)) # 100 since embedding_dimesion is 100, +1 because index 0 is reserved in word_index\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "# We can initialize random vector and assign for words which are not present in embeddings.Other option is keep trainable=true in embedding layer of the NN model.\n",
        "# We choose 2nd option"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6rN9WYgiomF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd655a19-7478-4950-98f8-5b0af4e820f9"
      },
      "source": [
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / len(word_index)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6503496503496503"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC-yKcH2iqkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "x_train, y_train = shuffle(train_sequences_padded, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMLd_nj2itGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44e8d6b6-829d-4e05-dee0-77071bbcd3b2"
      },
      "source": [
        "x_train = np.array(x_train[:])\n",
        "train_labels = [[1,0] if x == 1 else [0,1] for x in y_train[:]] \n",
        "y_train = np.array(train_labels[:])\n",
        "len(x_train),len(y_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10611, 10611)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98T0bztQiu2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbe7f51c-af34-4482-bc9b-251b03b6aad9"
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "net = Dropout(0.3)(embedded_sequences)\n",
        "net = Bidirectional(LSTM(200,recurrent_dropout=0.4))(net)\n",
        "net = Dropout(0.3)(net)\n",
        "output = Dense(2, activation = 'softmax')(net)\n",
        "model = Model(inputs = sequence_input, outputs = output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "# Keeping a checkpoint to store only the model which gives best output validation accuracy\n",
        "chkpt=ModelCheckpoint('expertiza_rnn_model.h5',monitor='val_acc',verbose=1,save_best_only=True)\n",
        "model_history = model.fit(x_train, y_train, batch_size=256, epochs=10, validation_split=0.2,callbacks=[chkpt])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 80, 100)           858100    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 80, 100)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 400)               481600    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 802       \n",
            "=================================================================\n",
            "Total params: 1,340,502\n",
            "Trainable params: 1,340,502\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 8488 samples, validate on 2123 samples\n",
            "Epoch 1/10\n",
            "8488/8488 [==============================] - 75s 9ms/step - loss: 0.5871 - acc: 0.6800 - val_loss: 0.5538 - val_acc: 0.7301\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.73010, saving model to expertiza_rnn_model.h5\n",
            "Epoch 2/10\n",
            "8488/8488 [==============================] - 72s 9ms/step - loss: 0.5231 - acc: 0.7491 - val_loss: 0.4653 - val_acc: 0.7984\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.73010 to 0.79840, saving model to expertiza_rnn_model.h5\n",
            "Epoch 3/10\n",
            "8488/8488 [==============================] - 72s 9ms/step - loss: 0.4798 - acc: 0.7812 - val_loss: 0.4658 - val_acc: 0.8031\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.79840 to 0.80311, saving model to expertiza_rnn_model.h5\n",
            "Epoch 4/10\n",
            "8488/8488 [==============================] - 72s 9ms/step - loss: 0.4630 - acc: 0.7908 - val_loss: 0.4244 - val_acc: 0.8224\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.80311 to 0.82242, saving model to expertiza_rnn_model.h5\n",
            "Epoch 5/10\n",
            "8488/8488 [==============================] - 72s 8ms/step - loss: 0.4547 - acc: 0.7967 - val_loss: 0.4246 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.82242 to 0.82289, saving model to expertiza_rnn_model.h5\n",
            "Epoch 6/10\n",
            "8488/8488 [==============================] - 72s 9ms/step - loss: 0.4425 - acc: 0.8040 - val_loss: 0.4126 - val_acc: 0.8267\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.82289 to 0.82666, saving model to expertiza_rnn_model.h5\n",
            "Epoch 7/10\n",
            "8488/8488 [==============================] - 72s 8ms/step - loss: 0.4381 - acc: 0.8022 - val_loss: 0.4050 - val_acc: 0.8262\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.82666\n",
            "Epoch 8/10\n",
            "8488/8488 [==============================] - 72s 9ms/step - loss: 0.4260 - acc: 0.8097 - val_loss: 0.4064 - val_acc: 0.8295\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.82666 to 0.82949, saving model to expertiza_rnn_model.h5\n",
            "Epoch 9/10\n",
            "8488/8488 [==============================] - 72s 9ms/step - loss: 0.4234 - acc: 0.8097 - val_loss: 0.3988 - val_acc: 0.8295\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.82949 to 0.82949, saving model to expertiza_rnn_model.h5\n",
            "Epoch 10/10\n",
            "8488/8488 [==============================] - 72s 9ms/step - loss: 0.4203 - acc: 0.8150 - val_loss: 0.3997 - val_acc: 0.8318\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.82949 to 0.83184, saving model to expertiza_rnn_model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srYh6onyiyb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "e69ab97e-df5b-4193-8b5b-5a7432b5ec5b"
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences_1 = embedding_layer(sequence_input)\n",
        "\n",
        "net1 = Dropout(0.3)(embedded_sequences_1)\n",
        "net1 = Conv1D(50, 3, padding='same', activation='relu')(net1)\n",
        "net1 = AveragePooling1D(pool_size=4)(net1)\n",
        "net1 = LSTM(100, recurrent_dropout=0.3)(net1)\n",
        "net1 = Dropout(0.2)(net1)\n",
        "output1 = Dense(2, activation='softmax')(net1)\n",
        "\n",
        "model5 = Model(inputs = sequence_input, outputs = output1)\n",
        "model5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model5.summary()\n",
        "\n",
        "# Keeping a checkpoint to store only the model which gives best output validation accuracy\n",
        "chkpt=ModelCheckpoint('expertiza_cnn_rnn_model.h5',monitor='val_acc',verbose=1,save_best_only=True)\n",
        "model_history1 = model5.fit(x_train, y_train, batch_size=100, epochs=10, validation_split=0.1,callbacks=[chkpt])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 80, 100)           858100    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 80, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 80, 50)            15050     \n",
            "_________________________________________________________________\n",
            "average_pooling1d_1 (Average (None, 20, 50)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 933,752\n",
            "Trainable params: 933,752\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 9549 samples, validate on 1062 samples\n",
            "Epoch 1/10\n",
            "9549/9549 [==============================] - 10s 1ms/step - loss: 0.5527 - acc: 0.7169 - val_loss: 0.4112 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.80979, saving model to expertiza_cnn_rnn_model.h5\n",
            "Epoch 2/10\n",
            "3800/9549 [==========>...................] - ETA: 5s - loss: 0.4359 - acc: 0.8137"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c0c3a459afac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Keeping a checkpoint to store only the model which gives best output validation accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mchkpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expertiza_cnn_rnn_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel_history1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchkpt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAye3pksi1WQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the model so that it can be loaded easily again\n",
        "model.save_weights('expertiza_rnn_model_weights.h5')\n",
        "\n",
        "# Save the model architecture\n",
        "with open('expertiza_rnn_model_architecture.json', 'w') as f:\n",
        "    f.write(model.to_json())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7tRfBzNi44D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Persisting model weights\n",
        "!cp expertiza_rnn_model_weights.h5 gdrive/My\\ Drive/AI-In-Peer-Assessment/model/\n",
        "# Persisting model architecture\n",
        "!cp expertiza_rnn_model_architecture.json gdrive/My\\ Drive/AI-In-Peer-Assessment/model/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}