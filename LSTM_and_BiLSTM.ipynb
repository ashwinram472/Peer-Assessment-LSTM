{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM and BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPm8sKtPrUhbGY4bmWOfvNt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinram472/Peer-Assessment-LSTM/blob/master/LSTM_and_BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2hu7kAlfUPb",
        "colab_type": "code",
        "outputId": "dee8213d-26e1-4f36-825a-e3dcead95e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lcIF9KafWty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp gdrive/My\\ Drive/AI-In-Peer-Assessment/problems_expertiza_randomized1.csv ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o9sKPMkhH2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# To store data\n",
        "import pandas as pd\n",
        "\n",
        "# To use regular expressions\n",
        "import re\n",
        "\n",
        "#To load and save data\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXdrqHxQhKIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = pd.read_csv(\"problems_expertiza_randomized1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvAy-t0yhLkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = reviews.filter([\"REVIEW\",\"TAG\"])\n",
        "data = data.loc[data.REVIEW.apply(lambda x: not isinstance(x, (float, int)))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRPY7ICfhass",
        "colab_type": "code",
        "outputId": "9fc10b92-3658-4b3e-ee29-9f4cf8a04f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data['TAG'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    5585\n",
              "0    5585\n",
              "Name: TAG, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYeL1wQKhiUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set, test_set = train_test_split(data, test_size=0.05, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mah52AEVhloL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_reviews = list(train_set[\"REVIEW\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELjaTwq0hoVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = list(train_set[\"TAG\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odHx7M8bhp0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(train_reviews)):\n",
        "  train_reviews[i] = re.sub('\\d',' ',train_reviews[i]) # Replacing digits by space\n",
        "  train_reviews[i] = re.sub(r'\\s+[a-z][\\s$]', ' ',train_reviews[i]) # Removing single characters and spaces alongside\n",
        "  train_reviews[i] = re.sub(r'\\s+', ' ',train_reviews[i]) # Replacing more than one space with a single space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0jn2A_5huDn",
        "colab_type": "code",
        "outputId": "8b4de3ce-7939-499d-8338-a174b200abf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for i in range(len(train_reviews)):\n",
        "    if 'www.' in train_reviews[i] or 'http:' in train_reviews[i] or 'https:' in train_reviews[i] or '.com' in train_reviews[i]:\n",
        "        train_reviews[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_reviews[i])\n",
        "        \n",
        "        \n",
        "train_reviews[1:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The deployed link is not accessible. The readme file describes the process for just admin login but not for the consequent steps.',\n",
              " 'Yes, it is reasonable as I dont��see any obvious issues with it. But the testing could be done better rather than checking the previoustest��cases that was already present.',\n",
              " 'Basic functional test cases are written for four controllers.',\n",
              " 'The team has only made one significant change to the following file: app/controllers/assignments_controller.rb . It is an if statement which is difficult to follow since the team has not written any comments for the same.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3SQNJdrhzLX",
        "colab_type": "code",
        "outputId": "824fd181-1985-4e85-996a-6f4305262ade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# Use Tokenizer to remove punctuations and non-word characters and tokenize the text\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='tensorflow' # Or TenserFlow\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOVPYMh1h7px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 80\n",
        "MAX_NB_WORDS = 80 # This specifies how many top tokens in each review to be stored. Wrongly interpreted as total number of words(token) together in whole dataset\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O4D6w1qh_pV",
        "colab_type": "code",
        "outputId": "7d97a3de-f271-4c01-8de5-950f60f9bb5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Takes 5 minutes to run on entire training dataset\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(train_reviews)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Number of Unique Tokens',len(word_index)) # Total 996497 unique words"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unique Tokens 8580\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeGAw9AUiCF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RApBkixlkLOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp gdrive/My\\ Drive/Glove/glove100d.txt ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pii11AwUiGc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('glove100d.txt')):\n",
        "  values = line.split() # 0 th index will be the word and rest will the embedding vector (size 100 as we have used Glove.6B.100D embedding file) \n",
        "  embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdGk43Ruim4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create token(words in word index)-embedding mapping\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100)) # 100 since embedding_dimesion is 100, +1 because index 0 is reserved in word_index\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "# We can initialize random vector and assign for words which are not present in embeddings.Other option is keep trainable=true in embedding layer of the NN model.\n",
        "# We choose 2nd option"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6rN9WYgiomF",
        "colab_type": "code",
        "outputId": "b427ca1b-0898-49fd-d83f-2395ae45aca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / len(word_index)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6503496503496503"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC-yKcH2iqkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "x_train, y_train = shuffle(train_sequences_padded, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMLd_nj2itGD",
        "colab_type": "code",
        "outputId": "40cc43dc-4829-405e-8ab5-0e2c1ca0d394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train = np.array(x_train[:])\n",
        "train_labels = [[1,0] if x == 1 else [0,1] for x in y_train[:]] \n",
        "y_train = np.array(train_labels[:])\n",
        "len(x_train),len(y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10611, 10611)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98T0bztQiu2r",
        "colab_type": "code",
        "outputId": "7eb4e899-921d-42eb-a82f-800281fc0b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "net = Dropout(0.3)(embedded_sequences)\n",
        "net = Bidirectional(LSTM(200,recurrent_dropout=0.4))(net)\n",
        "net = Dropout(0.3)(net)\n",
        "output = Dense(2, activation = 'softmax')(net)\n",
        "model = Model(inputs = sequence_input, outputs = output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "# Keeping a checkpoint to store only the model which gives best output validation accuracy\n",
        "chkpt=ModelCheckpoint('expertiza_rnn_model.h5',monitor='val_acc',verbose=1,save_best_only=True)\n",
        "model_history = model.fit(x_train, y_train, batch_size=256, epochs=10, validation_split=0.2,callbacks=[chkpt])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 80, 100)           858100    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 80, 100)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 400)               481600    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 802       \n",
            "=================================================================\n",
            "Total params: 1,340,502\n",
            "Trainable params: 1,340,502\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 8488 samples, validate on 2123 samples\n",
            "Epoch 1/10\n",
            "8488/8488 [==============================] - 76s 9ms/step - loss: 0.5950 - acc: 0.6740 - val_loss: 0.5342 - val_acc: 0.7405\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.74046, saving model to expertiza_rnn_model.h5\n",
            "Epoch 2/10\n",
            "8488/8488 [==============================] - 74s 9ms/step - loss: 0.5154 - acc: 0.7591 - val_loss: 0.4803 - val_acc: 0.7753\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.74046 to 0.77532, saving model to expertiza_rnn_model.h5\n",
            "Epoch 3/10\n",
            "8488/8488 [==============================] - 73s 9ms/step - loss: 0.4821 - acc: 0.7804 - val_loss: 0.4912 - val_acc: 0.7800\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.77532 to 0.78003, saving model to expertiza_rnn_model.h5\n",
            "Epoch 4/10\n",
            "8488/8488 [==============================] - 73s 9ms/step - loss: 0.4487 - acc: 0.7963 - val_loss: 0.4630 - val_acc: 0.7965\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.78003 to 0.79651, saving model to expertiza_rnn_model.h5\n",
            "Epoch 5/10\n",
            "8488/8488 [==============================] - 72s 9ms/step - loss: 0.4411 - acc: 0.8024 - val_loss: 0.4437 - val_acc: 0.8022\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.79651 to 0.80217, saving model to expertiza_rnn_model.h5\n",
            "Epoch 6/10\n",
            "8488/8488 [==============================] - 73s 9ms/step - loss: 0.4400 - acc: 0.8061 - val_loss: 0.4340 - val_acc: 0.8111\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.80217 to 0.81112, saving model to expertiza_rnn_model.h5\n",
            "Epoch 7/10\n",
            "8488/8488 [==============================] - 74s 9ms/step - loss: 0.4276 - acc: 0.8097 - val_loss: 0.4450 - val_acc: 0.8097\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.81112\n",
            "Epoch 8/10\n",
            "8488/8488 [==============================] - 74s 9ms/step - loss: 0.4189 - acc: 0.8192 - val_loss: 0.4425 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.81112\n",
            "Epoch 9/10\n",
            "8488/8488 [==============================] - 73s 9ms/step - loss: 0.4172 - acc: 0.8187 - val_loss: 0.4497 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.81112\n",
            "Epoch 10/10\n",
            "8488/8488 [==============================] - 74s 9ms/step - loss: 0.4164 - acc: 0.8182 - val_loss: 0.4317 - val_acc: 0.8083\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.81112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srYh6onyiyb9",
        "colab_type": "code",
        "outputId": "988035bf-7337-41ff-a122-94c716cc89c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences_1 = embedding_layer(sequence_input)\n",
        "\n",
        "net1 = Dropout(0.3)(embedded_sequences_1)\n",
        "net1 = Conv1D(50, 3, padding='same', activation='relu')(net1)\n",
        "net1 = AveragePooling1D(pool_size=4)(net1)\n",
        "net1 = LSTM(100, recurrent_dropout=0.3)(net1)\n",
        "net1 = Dropout(0.2)(net1)\n",
        "output1 = Dense(2, activation='softmax')(net1)\n",
        "\n",
        "model5 = Model(inputs = sequence_input, outputs = output1)\n",
        "model5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model5.summary()\n",
        "\n",
        "# Keeping a checkpoint to store only the model which gives best output validation accuracy\n",
        "chkpt=ModelCheckpoint('expertiza_cnn_rnn_model.h5',monitor='val_acc',verbose=1,save_best_only=True)\n",
        "model_history1 = model5.fit(x_train, y_train, batch_size=100, epochs=10, validation_split=0.1,callbacks=[chkpt])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 80, 100)           858100    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 80, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 80, 50)            15050     \n",
            "_________________________________________________________________\n",
            "average_pooling1d_1 (Average (None, 20, 50)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 933,752\n",
            "Trainable params: 933,752\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 9549 samples, validate on 1062 samples\n",
            "Epoch 1/10\n",
            "9549/9549 [==============================] - 11s 1ms/step - loss: 0.5507 - acc: 0.7247 - val_loss: 0.4659 - val_acc: 0.7881\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.78814, saving model to expertiza_cnn_rnn_model.h5\n",
            "Epoch 2/10\n",
            "9549/9549 [==============================] - 9s 962us/step - loss: 0.4306 - acc: 0.8081 - val_loss: 0.4394 - val_acc: 0.8004\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.78814 to 0.80038, saving model to expertiza_cnn_rnn_model.h5\n",
            "Epoch 3/10\n",
            "9549/9549 [==============================] - 9s 963us/step - loss: 0.4146 - acc: 0.8174 - val_loss: 0.4372 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.80038 to 0.80603, saving model to expertiza_cnn_rnn_model.h5\n",
            "Epoch 4/10\n",
            "9549/9549 [==============================] - 9s 952us/step - loss: 0.4090 - acc: 0.8219 - val_loss: 0.4295 - val_acc: 0.8032\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.80603\n",
            "Epoch 5/10\n",
            "9549/9549 [==============================] - 9s 979us/step - loss: 0.4064 - acc: 0.8252 - val_loss: 0.4396 - val_acc: 0.8032\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.80603\n",
            "Epoch 6/10\n",
            "9549/9549 [==============================] - 9s 953us/step - loss: 0.4001 - acc: 0.8280 - val_loss: 0.4374 - val_acc: 0.8145\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.80603 to 0.81450, saving model to expertiza_cnn_rnn_model.h5\n",
            "Epoch 7/10\n",
            "9549/9549 [==============================] - 9s 968us/step - loss: 0.3998 - acc: 0.8302 - val_loss: 0.4264 - val_acc: 0.8126\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.81450\n",
            "Epoch 8/10\n",
            "9549/9549 [==============================] - 9s 978us/step - loss: 0.3963 - acc: 0.8305 - val_loss: 0.4266 - val_acc: 0.8164\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.81450 to 0.81638, saving model to expertiza_cnn_rnn_model.h5\n",
            "Epoch 9/10\n",
            "9549/9549 [==============================] - 9s 964us/step - loss: 0.3958 - acc: 0.8309 - val_loss: 0.4272 - val_acc: 0.8154\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.81638\n",
            "Epoch 10/10\n",
            "9549/9549 [==============================] - 9s 953us/step - loss: 0.3926 - acc: 0.8329 - val_loss: 0.4280 - val_acc: 0.8154\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.81638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAye3pksi1WQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the model so that it can be loaded easily again\n",
        "model.save_weights('expertiza_rnn_model_weights.h5')\n",
        "\n",
        "# Save the model architecture\n",
        "with open('expertiza_rnn_model_architecture.json', 'w') as f:\n",
        "    f.write(model.to_json())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7tRfBzNi44D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Persisting model weights\n",
        "!cp expertiza_rnn_model_weights.h5 gdrive/My\\ Drive/AI-In-Peer-Assessment/model/\n",
        "# Persisting model architecture\n",
        "!cp expertiza_rnn_model_architecture.json gdrive/My\\ Drive/AI-In-Peer-Assessment/model/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}